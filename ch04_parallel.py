import asyncio
from utils import llm_call_async

# Define the list of questions and LLMs to process in parallel
question = ("Try to translate this sentence into natural korean.\n"
            "\"Do what you can, with what you have, where you are.\" - Theodore Roosevelt")

parallel_prompt_details = [
    {"user_prompt": question, "model": "gpt-4o"},
    {"user_prompt": question, "model": "gpt-4o-mini"},
    {"user_prompt": question, "model": "o3"},
]

# Define parallel processing function
async def run_llm_parallel(prompt_details):
    
    # Create a list of asynchronous LLM call tasks
    tasks = [
        llm_call_async(prompt['user_prompt'], prompt['model'])
        for prompt in prompt_details
    ]
    
    responses = []
    
    # Execute task list concurrently and collect results
    for task in asyncio.as_completed(tasks):
        result = await task
        print(result)
        responses.append(result)
        
    return responses


# Declare main function and execute parallel processing function
async def main():
    responses = await run_llm_parallel(parallel_prompt_details)
    
    aggregator_prompt = (
        "The following are responses generated by multiple LLMs for the user's question.\n"
        "Your role is to synthesize these responses and provide a final answer.\n"
        "Some responses may be inaccurate or biased, so please provide a reliable and accurate answer.\n"
        
        "Just return the final response.\n\n"
        "User Input:\n"
        f"{question}\n\n"
        "model response:"
    )
    
    for i in range(len(parallel_prompt_details)):
        aggregator_prompt += f"\n{i+1}. 모델 응답: {responses[i]}\n"
        
    print("---------- Final Prompt ----------\n", aggregator_prompt)
    
    final_response = await llm_call_async(aggregator_prompt, model="gpt-4o")
    print("---------- Final Translation ----------\n", final_response)
    
    
if __name__ == "__main__":
    asyncio.run(main())
    
    